{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1478,"status":"ok","timestamp":1674754821374,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"zbyDywULOw1z","outputId":"208dde46-b73c-4562-a484-6b24260d6da2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jan 26 17:40:20 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3533,"status":"ok","timestamp":1674754824903,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"Hkkxc8it5U7G","outputId":"a4fd8b96-ce3e-4c73-946d-c4d4ec16f87e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","NVIDIA A100-SXM4-40GB\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]}],"source":["import torch\n","\n","# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133535,"status":"ok","timestamp":1674754958433,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"WwwvyZTubIOm","outputId":"0151c6cf-5bf8-48f6-9595-e8025c4a9e4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"7SnT7YhEJFrG"},"source":["> # ðŸ”¶ **Pre-Settings:**\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"QPwDmuDmqKEN","executionInfo":{"status":"ok","timestamp":1674755138373,"user_tz":-210,"elapsed":18,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["#### --------- Settings --------\n","load_losses = True\n","load_model = True\n","load_epoch = 32\n","\n","checkpoint_path_loss = '/content/drive/MyDrive/Models/Checkpoints/MyVideoModel_Final_Residual_V3_BN_Seq/'\n","checkpoint_path_model = '/content/drive/MyDrive/Models/Checkpoints/MyVideoModel_Final_Residual_V3_BN_Seq/'\n","# ------------------------------\n","lr= 0.0002 # Learning rate"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1674754958435,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"6rNel-m8sMe9","outputId":"b5051bb4-1f9e-4ede-c4eb-8bb9c012d747"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1674754958436,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"QCA18JMetVab","outputId":"1c3a92ba-ac73-4df3-8770-274ab4a1578a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","NVIDIA A100-SXM4-40GB\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]}],"source":["import torch\n","\n","# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"]},{"cell_type":"markdown","metadata":{"id":"_ebAc19vIMRb"},"source":["> # ðŸ”¶ **Download & Un-Zip Datasets:**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"V-xBATaBH2Dl","executionInfo":{"status":"ok","timestamp":1674754958437,"user_tz":-210,"elapsed":11,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# !pip install --upgrade --no-cache-dir gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMv1iangz6SB"},"outputs":[],"source":["# !unzip RawFrames_360p_6Frames_CRF0_PNG_RGB.zip -d '/content/'\n","!unzip -u '/content/drive/Shareddrives/AI/Datasets/ExtractedFrames/Nature_Frames2/RawFrames_240p_6Frames_CRF13_PNG_RGB_Slow.zip' -d '/content/'\n","\n","# '/content/drive/MyDrive/AI/Datasets/ExtractedFrames/Nature_Frames2/RawFrames_240p_6Frames_CRF13_PNG_RGB_Slow.zip'"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"A4A27VNGvEeD","executionInfo":{"status":"ok","timestamp":1674755037745,"user_tz":-210,"elapsed":13,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# Download Decoded Frames (CRF40):\n","# !gdown 1-EFXCAaVuA-grCf__SmMugxyoUigVYur\n","# !unzip -u '/content/drive/MyDrive/AI/Datasets/ExtractedFrames/Nature_Frames2/ResidualFrames_240p_6Frames_CRF13_40_PNG_RGB_NoNorm.zip' -d '/content/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-o9XYVRIfEk"},"outputs":[],"source":["# !unzip CompressedFrames_360p_6Frames_CRF33_PNG_RGB.zip -d '/content/'\n","\n","!unzip -u '/content/drive/Shareddrives/AI/Datasets/ExtractedFrames/Nature_Frames2/CompressedFrames_240p_6Frames_CRF40_PNG_RGB.zip' -d '/content/'\n","\n","# '/content/drive/MyDrive/AI/Datasets/ExtractedFrames/Nature_Frames2/CompressedFrames_240p_6Frames_CRF40_PNG_RGB.zip'"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1674755126296,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"p691Z5csJs_w","outputId":"cccd18b0-727f-4ca8-e51f-45ae7aaa675a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Video-Compression-Tools'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (23/23), done.\u001b[K\n","remote: Total 24 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), 8.94 KiB | 2.23 MiB/s, done.\n","drive\timage_utils.py\tsample_data\n","Frames\t__pycache__\tVideo-Compression-Tools\n"]}],"source":["!git clone https://github.com/parhamzm/Video-Compression-Tools.git\n","\n","!cp '/content/Video-Compression-Tools/image_utils.py' './'\n","\n","from image_utils import plot_images, normalize_image\n","\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"BgJcAODtJva3"},"source":["# **Code:**"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"U3yFqptjJvD2","executionInfo":{"status":"ok","timestamp":1674755126297,"user_tz":-210,"elapsed":15,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# %reload_ext autoreload\n","# %autoreload 2\n","%matplotlib inline\n","\n","import math\n","import time\n","import os\n","import glob\n","import json\n","import ast\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","from torchvision.datasets import ImageFolder, DatasetFolder\n","\n","from google.colab import files\n","\n","# Our libraries\n","\n","# some initial setup\n","# np.set_printoptions(precision=2)\n","use_gpu = torch.cuda.is_available()\n","# np.random.seed(1234)\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3p-F7AxCJzSN","executionInfo":{"status":"ok","timestamp":1674755126297,"user_tz":-210,"elapsed":14,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["Train_decoded_DATA_DIR = '/content/Frames/CompressedFrames_CRF40/Train/'\n","Train_residual_DATA_DIR = '/content/Frames/ResidualFrames/Train/'\n","Train_raw_DATA_DIR = '/content/Frames/RawFrames/Train/'\n","\n","Test_decoded_DATA_DIR = '/content/Frames/CompressedFrames_CRF40/Test/'\n","Test_residual_DATA_DIR = '/content/Frames/ResidualFrames/Test/'\n","Test_raw_DATA_DIR = '/content/Frames/RawFrames/Test/'"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GWEtFfaAJ1mA","executionInfo":{"status":"ok","timestamp":1674755126297,"user_tz":-210,"elapsed":13,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["import os\n","from skimage import io\n","import pandas as pd\n","from torchvision.io import read_image\n","\n","\n","\n","class MyDataset(Dataset):\n","    def __init__(self, datasetA_dir, datasetB_dir, transform=None):\n","        self.datasetA_dir = datasetA_dir\n","        self.datasetB_dir = datasetB_dir\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        # print(\"index: \", index)\n","        # print(\"dirA: \", self.datasetA_dir)\n","        # print(\"dirB: \", self.datasetB_dir)\n","        # print(\"LenA: \", len(self.datasetA_dir))\n","        # print(\"LenB: \", len(self.datasetB_dir))\n","        digA = \"compframe\" + (\"0\" * (5 - len(str(index)))) + str(index) + \".png\"\n","        dirA = os.path.join(self.datasetA_dir, digA)\n","        digB = \"rawframe\" + (\"0\" * (5 - len(str(index)))) + str(index) + \".png\"\n","        dirB = os.path.join(self.datasetB_dir, digB)\n","        # xA = self.datasetA_dir[index]\n","        # xB = self.datasetB_dir[index]\n","        imageA = io.imread(dirA)\n","        imageB = io.imread(dirB)\n","        if self.transform:\n","            imageA = self.transform(imageA)\n","            imageB = self.transform(imageB)\n","        res = {'RawIMG': imageB, 'RawName': digB, 'CompIMG': imageA, 'CompName': digA}\n","        # return imageA, imageB\n","        return res\n","    \n","    def __len__(self):\n","        path, dirs, filesA = next(os.walk(self.datasetA_dir))\n","        # path, dirs, filesB = next(os.walk(self.datasetB_dir))\n","        file_countA = len(filesA)\n","        # file_countB = len(filesB)\n","        # print(\"File CountA: \", file_countA)\n","        # print(\"File CountB: \", file_countB)\n","        # print(\"Len Decoded:\", len(self.datasetA_dir))\n","        # print(\"Len Raw:\", len(self.datasetB_dir))\n","        return file_countA\n","\n","\n","transform_train = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(3),\n","    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.RandomApply([\n","      transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8\n","    ),\n","    transforms.RandomGrayscale(0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n","])\n","\n","train_dataset = MyDataset(Train_decoded_DATA_DIR, Train_raw_DATA_DIR, transform=transforms.Compose([\n","                                transforms.ToTensor(),\n","                              #   torchvision.transforms.Normalize(\n","                              #     mean=[0.5, 0.5, 0.5],\n","                              #     std=[0.5, 0.5, 0.5],\n","                              # ),\n","                            ])\n",")\n","\n","test_dataset = MyDataset(Test_decoded_DATA_DIR, Test_raw_DATA_DIR, transform=transforms.Compose([\n","                                transforms.ToTensor(),\n","                              #   torchvision.transforms.Normalize(\n","                              #     mean=[0.5, 0.5, 0.5],\n","                              #     std=[0.5, 0.5, 0.5],\n","                              # ),\n","                            ])\n",")\n","\n","#   torchvision.transforms.Normalize(\n","                              #     mean=[0.485, 0.456, 0.406],\n","                              #     std=[0.229, 0.224, 0.225],\n","                              # ),"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1674755126297,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"gl1aE9UNJ3CX","outputId":"6cabe8a5-15c5-4124-cebb-3691492eca35"},"outputs":[{"output_type":"stream","name":"stdout","text":["41400\n"]}],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","# print(len(train_dataset.))\n","# print(len(train_raw_dataset.samples))\n","# print(len(test_decoded_dataset.samples))\n","# print(len(test_raw_dataset.samples))\n","# print(len(train_raw_dataset))\n","# print(len(train_decoded_dataset))\n","print(len(train_dataset))"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"pSG5lQX-JN8x","executionInfo":{"status":"ok","timestamp":1674755126297,"user_tz":-210,"elapsed":12,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# my_mse = nn.MSELoss()\n","# my_mae = nn.L1Loss()\n","\n","# no_dae_loss = []\n","# for noise_batch, image_batch in tqdm(train_loader): #zip(train_decoded_loader, train_raw_loader):\n","#     loss1 = my_mae(noise_batch.to(device), image_batch.to(device))\n","#     # loss2 = my_mae(noise_batch, image_batch)\n","#     loss =  loss1\n","#     no_dae_loss.append(loss.item())\n","# print(\"Train Loss without DAE:=> {:.20f}\".format(np.mean(no_dae_loss)))\n","\n","\n","# no_dae_loss = []\n","# for noise_batch, image_batch in tqdm(test_loader): #zip(test_decoded_loader, test_raw_loader):\n","#     loss1 = my_mae(noise_batch.to(device), image_batch.to(device))\n","#     # loss2 = my_mae(noise_batch, image_batch)\n","#     loss = loss1\n","#     no_dae_loss.append(loss.item())\n","# print(\"Test Loss without DAE:=> {:.20f}\".format(np.mean(no_dae_loss)))\n","\n","# # Train Loss without DAE:=> 0.02978461904001336591\n","# # Test Loss without DAE:=> 0.03176539410450028578"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"WpSokaomUdvk","executionInfo":{"status":"ok","timestamp":1674755126297,"user_tz":-210,"elapsed":12,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# batch1, batch2 = next(iter(train_loader))\n","\n","# train_data = batch1\n","# train_data2 = batch2\n","\n","# classes = np.zeros(len(train_data)+1, dtype='int32') #0 #train_data.classes\n","\n","# # plot_images(batch1, classes, classes)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"K5qz962yr_N1","executionInfo":{"status":"ok","timestamp":1674755126298,"user_tz":-210,"elapsed":12,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# plot_images(batch2, classes, classes)"]},{"cell_type":"markdown","metadata":{"id":"kwsG5HBNToph"},"source":["# **U-Net Class:**"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"3OPtKpaKq9e4","executionInfo":{"status":"ok","timestamp":1674755126298,"user_tz":-210,"elapsed":12,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        # torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        # torch.nn.init.uniform_(m.weight.data, 0., 0.002)\n","        # torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='leaky_relu', mode='fan_in')\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        # torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        # torch.nn.init.uniform_(m.weight.data, 0., 0.002)\n","        # torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='leaky_relu', mode='fan_in')\n","        # torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","\n","def initialize_weights(m):\n","  if isinstance(m, nn.Conv2d):\n","      nn.init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n","      # torch.nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n","      # torch.nn.init.uniform_(m.weight.data, 0., 0.02)\n","      if m.bias is not None:\n","          nn.init.constant_(m.bias.data, 0)\n","  elif isinstance(m, nn.BatchNorm2d):\n","      nn.init.constant_(m.weight.data, 1)\n","      nn.init.constant_(m.bias.data, 0)\n","  elif isinstance(m, nn.Linear):\n","      nn.init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n","      # torch.nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n","      # torch.nn.init.uniform_(m.weight.data, 0., 0.02)\n","      nn.init.constant_(m.bias.data, 0)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"da_Ohuso_r_T","executionInfo":{"status":"ok","timestamp":1674755126298,"user_tz":-210,"elapsed":12,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["#########################################\n","#           U-NET -             #\n","#########################################\n","def crop(image, new_shape):\n","    '''\n","    Function for cropping an image tensor: Given an image tensor and the new shape,\n","    crops to the center pixels (assumes that the input's size and the new size are\n","    even numbers).\n","    Parameters:\n","        image: image tensor of shape (batch size, channels, height, width)\n","        new_shape: a torch.Size object with the shape you want x to have\n","    '''\n","    # There are many ways to implement this crop function, but it's what allows\n","    # the skip connection to function as intended with two differently sized images!\n","    #### START CODE HERE ####\n","    middle_height = image.shape[2] // 2\n","    middle_width = image.shape[3] // 2\n","    starting_height = middle_height - round(new_shape[2] / 2)\n","    final_height = starting_height + new_shape[2]\n","    starting_width = middle_width - round(new_shape[3] / 2)\n","    final_width = starting_width + new_shape[3]\n","    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n","    return cropped_image\n","\n","class ResidualBlock(nn.Module):\n","    '''\n","    ResidualBlock Class:\n","    Performs two convolutions and an instance normalization, the input is added\n","    to this output to form the residual block output.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def conv_block(self, in_features, kernal_size, padding, use_bn=False, activation='celu'):\n","        activations = nn.ModuleDict([\n","                ['lrelu', nn.LeakyReLU(0.2, inplace=True)],\n","                ['relu', nn.ReLU(inplace=True)],\n","                ['celu', torch.nn.CELU(alpha=1.0, inplace=True)],\n","                ['rrelu', nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=True)],\n","        ])\n","        if use_bn:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features, kernel_size=kernal_size, padding=padding, padding_mode='reflect'),\n","              nn.InstanceNorm2d(in_features),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features, in_features, kernel_size=kernal_size, padding=padding, padding_mode='reflect'),\n","              nn.InstanceNorm2d(in_features),\n","            ]\n","        else:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features, kernel_size=kernal_size, padding=padding, padding_mode='reflect'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features, in_features, kernel_size=kernal_size, padding=padding, padding_mode='reflect'),\n","            ]\n","        block = torch.nn.Sequential(*layers)\n","        return block\n","\n","    def __init__(self, input_channels, k=7, p=3, use_bn=True):\n","        super(ResidualBlock, self).__init__()\n","        self.residual_layer1 = self.conv_block(in_features=input_channels, kernal_size=k, padding=p, use_bn=use_bn)\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ResidualBlock: \n","        Given an image tensor, completes a residual block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        original_x = x.clone()\n","        x = self.residual_layer1(x)\n","        return original_x + x\n","\n","\n","# GRADED CLASS: ContractingBlock\n","class ContractingBlock(nn.Module):\n","    '''\n","    ContractingBlock Class\n","    Performs two convolutions followed by a max pool operation.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def conv_block(self, in_features, kernal_size, padding, use_bn=False, max_pool=False, activation='lrelu'):\n","        activations = nn.ModuleDict([\n","                ['lrelu', nn.LeakyReLU(0.2, inplace=True)],\n","                ['relu', nn.ReLU(inplace=True)],\n","                ['celu', torch.nn.CELU(alpha=1.0, inplace=True)],\n","                ['rrelu', nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=True)],\n","        ])\n","        if use_bn:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features * 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features * 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features * 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features * 2),\n","              activations[activation],\n","            ]\n","        else:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features * 2, in_features * 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","            ]\n","        if max_pool:\n","            layers.append(\n","                nn.Conv2d(in_features * 2, in_features * 2, kernel_size=2, stride=2, padding=0, padding_mode='zeros') # k+1, p\n","            )\n","        block = torch.nn.Sequential(*layers)\n","        return block\n","\n","    def __init__(self, input_channels, use_dropout=False, use_bn=True, use_max_pool=True, k=7, p=3):\n","        super(ContractingBlock, self).__init__()\n","        # You want to double the number of channels in the first convolution\n","        # and keep the same number of channels in the second.\n","        #### START CODE HERE ####\n","        self.contracting_layer1 = self.conv_block(in_features=input_channels, kernal_size=k, padding=p, use_bn=use_bn, max_pool=use_max_pool)\n","        # if use_dropout:\n","        #     self.dropout = nn.Dropout(p=0.3, inplace=True)\n","        #### END CODE HERE ####\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of ContractingBlock: \n","        Given an image tensor, completes a contracting block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.contracting_layer1(x)\n","        \n","        return x\n","\n","\n","\n","# CLASS: ExpandingBlock\n","class ExpandingBlock(nn.Module):\n","    '''\n","    ExpandingBlock Class\n","    Performs an upsampling, a convolution, a concatenation of its two inputs,\n","    followed by two more convolutions.\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","    '''\n","    def conv_block1(self, in_features, kernal_size, padding, upsample=False):\n","        activations = nn.ModuleDict([\n","                ['lrelu', nn.LeakyReLU(0.2, inplace=True)],\n","                ['relu', nn.ReLU(inplace=True)],\n","                ['celu', torch.nn.CELU(alpha=1.0, inplace=True)],\n","                ['rrelu', nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=True)],\n","        ])\n","        if upsample:\n","            layers_part1 = [\n","                nn.ConvTranspose2d(in_features, in_features // 2, kernel_size=2, stride=2, padding=0) # k+1, p\n","            ]\n","        else:\n","            layers_part1 = [\n","                nn.Conv2d(in_features, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros')\n","            ]\n","            \n","        block1 = torch.nn.Sequential(*layers_part1)\n","        return block1\n","\n","    def conv_block2(self, in_features, kernal_size, padding, use_bn=False, activation='celu'):\n","        activations = nn.ModuleDict([\n","                ['lrelu', nn.LeakyReLU(0.2, inplace=True)],\n","                ['relu', nn.ReLU(inplace=True)],\n","                ['celu', torch.nn.CELU(alpha=1.0, inplace=True)],\n","                ['rrelu', nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=True)],\n","        ])\n","        if use_bn:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features // 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features // 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features // 2),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              nn.InstanceNorm2d(in_features // 2),\n","              activations[activation],\n","            ]\n","        else:\n","            layers = [\n","              torch.nn.Conv2d(in_features, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","              torch.nn.Conv2d(in_features // 2, in_features // 2, kernel_size=kernal_size, padding=padding, padding_mode='zeros'),\n","              activations[activation],\n","            ]\n","        \n","        block2 = torch.nn.Sequential(*layers)\n","        return block2\n","\n","    def __init__(self, input_channels, use_dropout=False, use_bn=False, use_trans_conv=True, k=7, p=3):\n","        super(ExpandingBlock, self).__init__()\n","        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n","        # self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        self.expanding_layer1 = self.conv_block1(in_features=input_channels, kernal_size=k, padding=p, upsample=use_trans_conv)\n","        # \"followed by a 2x2 convolution that halves the number of feature channels\"\n","        # \"a concatenation with the correspondingly cropped feature map from the contracting path\"\n","        # \"and two 3x3 convolutions\"\n","        self.expanding_layer2 = self.conv_block2(in_features=input_channels, kernal_size=k, padding=p, use_bn=use_bn)\n","        #### START CODE HERE ####\n","        # if use_dropout:\n","        #     self.dropout = nn.Dropout()\n","        # self.use_dropout = use_dropout\n","        #### END CODE HERE ####\n","        # \"each followed by a ReLU\"\n"," \n","    def forward(self, x, skip_con_x):\n","        '''\n","        Function for completing a forward pass of ExpandingBlock: \n","        Given an image tensor, completes an expanding block and returns the transformed tensor.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","            skip_con_x: the image tensor from the contracting path (from the opposing block of x)\n","                    for the skip connection\n","        '''\n","        x = self.expanding_layer1(x)\n","        # print(\"Shape before cat:==> \", x.shape)\n","        # skip_con_x = crop(skip_con_x, x.shape)\n","        x = torch.cat([x, skip_con_x], axis=1)\n","        # print(\"Shape after cat:==> \", x.shape)\n","        x = self.expanding_layer2(x)\n","        return x\n","\n","\n","\n","# CLASS: FeatureMapBlock\n","class FeatureMapBlockPre(nn.Module):\n","    '''\n","    FeatureMapBlock Class\n","    The final layer of a Generator - \n","    maps each the output to the desired number of output channels\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels, k=11, p=5):\n","        super(FeatureMapBlockPre, self).__init__()\n","        self.conv0 = nn.Conv2d(input_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","        self.conv1 = nn.Conv2d(output_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","        # self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","        self.activation_func = torch.nn.CELU(alpha=1.0, inplace=True)\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of FeatureMapBlock: \n","        Given an image tensor, returns it mapped to the desired number of channels.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        # original_x = x.clone()\n","        # x = torch.cat([x, x_pre], axis=1)\n","        x = self.conv0(x)\n","        x = self.activation_func(x)\n","        x = self.conv1(x)\n","        # x = self.conv2(x)\n","        return x\n","\n","\n","class FeatureMapBlock(nn.Module):\n","    '''\n","    FeatureMapBlock Class\n","    The final layer of a Generator - \n","    maps each the output to the desired number of output channels\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels, output_channels, k=11, p=5):\n","        super(FeatureMapBlock, self).__init__()\n","        self.conv0 = nn.Conv2d(input_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","        # self.conv1 = nn.Conv2d(output_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","        # self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size=k, padding=p, padding_mode='reflect')\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of FeatureMapBlock: \n","        Given an image tensor, returns it mapped to the desired number of channels.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        x = self.conv0(x)\n","        # x = self.conv1(x)\n","        # x = self.conv2(x)\n","        return x"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"c64kUWGGToDc","executionInfo":{"status":"ok","timestamp":1674755126298,"user_tz":-210,"elapsed":11,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# GRADED CLASS: UNet\n","class UNet(nn.Module):\n","    '''\n","    UNet Class\n","    A series of 4 contracting blocks followed by 4 expanding blocks to \n","    transform an input image into the corresponding paired image, with an upfeature\n","    layer at the start and a downfeature layer at the end\n","    Values:\n","        input_channels: the number of channels to expect from a given input\n","        output_channels: the number of channels to expect for a given output\n","    '''\n","    def __init__(self, input_channels=3, output_channels=3, hidden_channels=4):\n","        super(UNet, self).__init__()\n","        # \"Every step in the expanding path consists of an upsampling of the feature map\"\n","        self.upfeature_pre = FeatureMapBlockPre(input_channels, input_channels)\n","        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n","        self.contract1 = ContractingBlock(hidden_channels, use_dropout=False, use_bn=True, use_max_pool=False)\n","        self.contract2 = ContractingBlock(hidden_channels * 2, use_dropout=False, use_bn=True, use_max_pool=False)\n","        self.contract3 = ContractingBlock(hidden_channels * 4, use_dropout=False, use_bn=True, use_max_pool=True)\n","        self.contract4 = ContractingBlock(hidden_channels * 8, use_dropout=False, use_bn=True, use_max_pool=False)\n","        self.contract5 = ContractingBlock(hidden_channels * 16, use_dropout=False, use_bn=True, use_max_pool=True)\n","        self.contract6 = ContractingBlock(hidden_channels * 32, use_dropout=False, use_bn=True, use_max_pool=False)\n","        res_mult = 64\n","        self.res0 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res1 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res2 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res3 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res4 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res5 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res6 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res7 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.res8 = ResidualBlock(hidden_channels * res_mult, use_bn=True)\n","        self.expand0 = ExpandingBlock(hidden_channels * 64, use_dropout=False, use_bn=True, use_trans_conv=False)\n","        self.expand1 = ExpandingBlock(hidden_channels * 32, use_dropout=False, use_bn=True, use_trans_conv=True)\n","        self.expand2 = ExpandingBlock(hidden_channels * 16, use_dropout=False, use_bn=True, use_trans_conv=False)\n","        self.expand3 = ExpandingBlock(hidden_channels * 8, use_dropout=False, use_bn=True, use_trans_conv=True)\n","        self.expand4 = ExpandingBlock(hidden_channels * 4, use_dropout=False, use_bn=True, use_trans_conv=False)\n","        self.expand5 = ExpandingBlock(hidden_channels * 2, use_dropout=False, use_bn=True, use_trans_conv=False)\n","        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n","        self.res_end1 = ResidualBlock(output_channels, k=11, p=5, use_bn=True)\n","        self.res_end2 = ResidualBlock(output_channels, k=11, p=5, use_bn=True)\n","        self.res_end3 = ResidualBlock(output_channels, k=11, p=5, use_bn=True)\n","        self.res_end4 = ResidualBlock(output_channels, k=11, p=5, use_bn=True)\n","        self.res_end5 = ResidualBlock(output_channels, k=11, p=5, use_bn=True)\n","        # self.tanh = torch.nn.Tanh()\n","        self.soft_sign = torch.nn.Softsign()\n","        # self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        '''\n","        Function for completing a forward pass of UNet: \n","        Given an image tensor, passes it through U-Net and returns the output.\n","        Parameters:\n","            x: image tensor of shape (batch size, channels, height, width)\n","        '''\n","        # Keep in mind that the expand function takes two inputs, \n","        # both with the same number of channels. \n","        #### START CODE HERE ####\n","        # x_pre = x + y_pre\n","        x = self.upfeature_pre(x)\n","        x0 = self.upfeature(x)\n","        x1 = self.contract1(x0)\n","        x2 = self.contract2(x1)\n","        x3 = self.contract3(x2)\n","        x4 = self.contract4(x3)\n","        x5 = self.contract5(x4)\n","        x6 = self.contract6(x5)\n","        y3 = self.res0(x6)\n","        y4 = self.res1(y3)\n","        y5 = self.res2(y4)\n","        y6 = self.res3(y5)\n","        y7 = self.res4(y6)\n","        y8 = self.res5(y7)\n","        y9 = self.res6(y8)\n","        y10 = self.res7(y9)\n","        y11 = self.res8(y10)\n","        x7 = self.expand0(y11, x5)\n","        x8 = self.expand1(x7, x4)\n","        x9 = self.expand2(x8, x3)\n","        x10 = self.expand3(x9, x2)\n","        x11 = self.expand4(x10, x1)\n","        x12 = self.expand5(x11, x0)\n","        xn = self.downfeature(x12)\n","        xn = self.res_end1(xn)\n","        xn = self.res_end2(xn)\n","        xn = self.res_end3(xn)\n","        xn = self.res_end4(xn)\n","        xn = self.res_end5(xn)\n","        # xn = self.sigmoid(xn)\n","        xn = self.soft_sign(xn)\n","        #### END CODE HERE ####\n","        return xn"]},{"cell_type":"markdown","metadata":{"id":"r3qmP0qBMzjo"},"source":["# **Initialize Model:**"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"MPWOxuAPKmes","executionInfo":{"status":"ok","timestamp":1674755138369,"user_tz":-210,"elapsed":12082,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["\n","### Set the random seed for reproducible results\n","# torch.manual_seed(0)\n","\n","### Initialize the two networks\n","# del u_net\n","x_pre_decoded = torch.zeros(1, 3, 240, 320).to(device)\n","u_net = UNet(input_channels=3, output_channels=3)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1674755138370,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"1zi73ibQy5P8","outputId":"33bde08a-d2fd-4977-cf11-98b4d7ad2c70"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["UNet(\n","  (upfeature_pre): FeatureMapBlockPre(\n","    (conv0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","    (conv1): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","    (activation_func): CELU(alpha=1.0, inplace=True)\n","  )\n","  (upfeature): FeatureMapBlock(\n","    (conv0): Conv2d(3, 4, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","  )\n","  (contract1): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(4, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (contract2): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(8, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (contract3): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(16, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (12): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (contract4): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (contract5): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (12): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (contract6): ContractingBlock(\n","    (contracting_layer1): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (6): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (9): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","  )\n","  (res0): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res1): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res2): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res3): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res4): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res5): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res6): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res7): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res8): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflect)\n","      (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (expand0): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): Conv2d(256, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(256, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (expand1): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (expand2): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (expand3): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (expand4): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): Conv2d(16, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(16, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (expand5): ExpandingBlock(\n","    (expanding_layer1): Sequential(\n","      (0): Conv2d(8, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","    )\n","    (expanding_layer2): Sequential(\n","      (0): Conv2d(8, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (4): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (5): CELU(alpha=1.0, inplace=True)\n","      (6): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (7): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (8): CELU(alpha=1.0, inplace=True)\n","      (9): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n","      (10): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (11): CELU(alpha=1.0, inplace=True)\n","    )\n","  )\n","  (downfeature): FeatureMapBlock(\n","    (conv0): Conv2d(4, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","  )\n","  (res_end1): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (4): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res_end2): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (4): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res_end3): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (4): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res_end4): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (4): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (res_end5): ResidualBlock(\n","    (residual_layer1): Sequential(\n","      (0): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (1): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","      (2): CELU(alpha=1.0, inplace=True)\n","      (3): Conv2d(3, 3, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), padding_mode=reflect)\n","      (4): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","    )\n","  )\n","  (soft_sign): Softsign()\n",")"]},"metadata":{},"execution_count":23}],"source":["u_net.apply(initialize_weights)\n","# # Move both the encoder and the decoder to the selected device\n","u_net.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1674755138370,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"5FJyvTCpeEEe","outputId":"2c8c7efb-eae8-4918-a3f3-51751801bff7"},"outputs":[{"output_type":"stream","name":"stdout","text":["===================================================\n","Number of Parameters:==>  79978123\n","===================================================\n"]}],"source":["pytorch_total_params = sum(p.numel() for p in u_net.parameters())\n","print(\"===================================================\")\n","print(\"Number of Parameters:==> \", pytorch_total_params)\n","print(\"===================================================\")\n"]},{"cell_type":"markdown","metadata":{"id":"wWaVY_lUM6_4"},"source":["# **Loss & Optimizer:**"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1674755138371,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"fBKGiUTLbjm2","outputId":"f73b42e1-8136-414e-9cd1-33b1eeba8d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selected device:=> cuda\n"]}],"source":["### Define the loss function\n","res_loss_fn_mse = torch.nn.MSELoss()\n","\n","res_loss_fn_mae = torch.nn.L1Loss()\n","\n","### Define an optimizer (both for the encoder and the decoder!)\n","\n","params_to_optimize = [\n","    # {'params': encoder.parameters()},\n","    {'params': u_net.parameters()}\n","]\n","\n","print(f'Selected device:=> {device}')\n","\n","unet_optimizer = torch.optim.Adam(u_net.parameters(), lr=lr)\n","# unet_optimizer = torch.optim.SGD(u_net.parameters(), lr=lr, momentum=0.9, weight_decay=0, dampening=0)"]},{"cell_type":"markdown","metadata":{"id":"g4shH3zTOaE5"},"source":["> # ðŸ”¶ **Train Model:**"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"IxSr_01BOgIp","executionInfo":{"status":"ok","timestamp":1674755537278,"user_tz":-210,"elapsed":523,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["class AverageMeter(object):\n","  \"\"\"Computes and stores the average and current value\"\"\"\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.val = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","  \n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count\n","\n","\n","### Training function\n","def train_epoch_den(epoch=0):\n","    # Set train mode for both the encoder and the decoder\n","    u_net.train()\n","\n","    loss_total = AverageMeter()\n","    # my_decoded_data = next(iter(train_loader))\n","    my_pre_x = torch.zeros(1, 3, 240, 320).to(device) # torch.zeros_like(my_decoded_data['RawIMG']).to(device)\n","    # decoded_data = 0\n","    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","    for my_iter, (batch) in enumerate(tqdm(train_loader)): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n","        # Move tensor to the proper device\n","        noisy_batch = batch['CompIMG'].to(device)\n","        raw_batch = batch['RawIMG'].to(device)\n","        unet_optimizer.zero_grad()\n","        # U-Net data\n","        # print(\"My Iter:==> \", my_iter)\n","        decoded_data = u_net(noisy_batch + my_pre_x)\n","        res_loss = res_loss_fn_mae((decoded_data + noisy_batch + my_pre_x), raw_batch)\n","        my_pre_x = decoded_data.detach()\n","        # Evaluate loss\n","        # res_loss_mse = res_loss_fn_mse((raw_batch - noisy_batch), decoded_data)\n","        \n","        # res_loss = res_loss_mae\n","        # Backward pass\n","        res_loss.backward()\n","        unet_optimizer.step()\n","        # Print batch loss\n","        # print('\\t partial train loss (single batch): %f' % (loss.data))\n","        loss_total.update(res_loss)\n","        writer.add_scalar('Iteration_Loss/train', loss_total.avg.item(), epoch * len(train_loader) + my_iter)\n","\n","    writer.add_scalar('Epoch_Loss/train', loss_total.avg.item(), epoch)\n","\n","    return loss_total.avg.item() # np.mean(res_train_loss)\n","\n","\n","### Testing function\n","def test_epoch_den(epoch):\n","    # Set evaluation mode for encoder and decoder\n","    u_net.eval()\n","    loss_total = AverageMeter()\n","    # my_decoded_data = next(iter(test_loader))\n","    # decoded_data = torch.zeros_like(my_decoded_data['RawIMG']).to(device)\n","    my_pre_x = torch.zeros(1, 3, 240, 320).to(device)\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        # with tqdm(test_loader, unit=\"batch\") as tepoch:\n","        for my_iter, (batch) in enumerate(tqdm(test_loader)): #zip(tqdm(test_decoded_loader), test_raw_loader):\n","            # Move tensor to the proper device\n","            noisy_batch = batch['CompIMG'].to(device)\n","            raw_batch = batch['RawIMG'].to(device)\n","            # Encode data\n","            decoded_data = u_net(noisy_batch + my_pre_x)\n","            res_loss = res_loss_fn_mae((decoded_data + noisy_batch + my_pre_x), raw_batch)\n","            my_pre_x = decoded_data.detach()\n","            # Append the network output and the original image to the lists\n","            # res_loss_mse = res_loss_fn_mse((raw_batch - noisy_batch), decoded_data)\n","            loss_total.update(res_loss)\n","            writer.add_scalar('Iteration_Loss/test', loss_total.avg.item(), epoch * len(test_loader) + my_iter)\n","\n","    writer.add_scalar('Epoch_Loss/test', loss_total.avg.item(), epoch)\n","\n","    return loss_total.avg.item()\n","\n","\n","def test_epoch_den_time(epoch):\n","    # Set evaluation mode for encoder and decoder\n","    u_net.eval()\n","    loss_total = AverageMeter()\n","    # my_decoded_data = next(iter(test_loader))\n","    # decoded_data = torch.zeros_like(my_decoded_data['RawIMG']).to(device)\n","    my_pre_x = torch.zeros(1, 3, 240, 320).to(device)\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        # with tqdm(test_loader, unit=\"batch\") as tepoch:\n","        for my_iter, (batch) in enumerate(tqdm(test_loader)): #zip(tqdm(test_decoded_loader), test_raw_loader):\n","            # Move tensor to the proper device\n","            noisy_batch = batch['CompIMG'].to(device)\n","            raw_batch = batch['RawIMG'].to(device)\n","\n","            # record start time\n","            start = time.process_time()\n","\n","            # Encode data\n","            decoded_data = u_net(noisy_batch + my_pre_x)\n","            # res_loss = res_loss_fn_mae((decoded_data + noisy_batch + my_pre_x), raw_batch)\n","            my_pre_x = decoded_data.detach()\n","            # record end time\n","            end = time.process_time()\n","\n","            # print elapsed time in seconds\n","            print(\"Elapsed time using process_time()\", (end - start) * 10**3, \"ms.\")\n","            # Append the network output and the original image to the lists\n","            # res_loss_mse = res_loss_fn_mse((raw_batch - noisy_batch), decoded_data)\n","            # loss_total.update(res_loss)\n","            # writer.add_scalar('Iteration_Loss/test', loss_total.avg.item(), epoch * len(test_loader) + my_iter)\n","\n","    # writer.add_scalar('Epoch_Loss/test', loss_total.avg.item(), epoch)\n","\n","    return loss_total.avg.item()\n","\n","# train_epoch_den()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"n7lGKMHNzEhc","executionInfo":{"status":"ok","timestamp":1674755539968,"user_tz":-210,"elapsed":7,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# torch.cuda.memory_summary(device=None, abbreviated=False)\n","# torch.cuda.empty_cache()\n","\n","def plot_ae_outputs_den(my_model, my_loader, device, n=1):\n","    plt.figure(figsize=(12, 30))\n","    my_model.eval()\n","    for i in range(n):\n","        ax = plt.subplot(6, n, i+1)\n","        batch = next(iter(my_loader)) #test_raw_dataset[i][0].unsqueeze(0)\n","        org_img = batch['RawIMG'][i].to(device)\n","        image_noisy = batch['CompIMG'][i].to(device)\n","        with torch.no_grad():\n","            image_noisy = image_noisy.to(device)\n","            residual_img = my_model(image_noisy.view(1, 3, 240, 320))\n","\n","        # new_data = ndimage.rotate(img.cpu().squeeze().numpy().T, angle, reshape=True)\n","        # ax.imshow((org_img.detach().permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8));\n","        ax.imshow(normalize_image(org_img).detach().permute(1, 2, 0).cpu().numpy());\n","      \n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False)\n","        ax.get_yaxis().set_visible(False)\n","        if i == n//2:\n","            ax.set_title('Original image', color=\"red\", fontweight=\"bold\")\n","        ax = plt.subplot(6, n, i + 1 + n);\n","        # new_data = ndimage.rotate(image_noisy.cpu().squeeze().numpy().T, angle, reshape=True)\n","        # ax.imshow((image_noisy.detach().permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n","        ax.imshow(normalize_image(image_noisy).permute(1, 2, 0).cpu().numpy());\n","        # plt.imshow(image_noisy.cpu().reshape(256, 512, 3))\n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False)\n","        ax.get_yaxis().set_visible(False)\n","        if i == n//2:\n","            ax.set_title('Corrupted image', color=\"red\", fontweight=\"bold\")\n","        # print(torch.max((org_img - image_noisy)))\n","        # print(\"Calc Res:=> \", torch.max(residual_img))\n","        ax = plt.subplot(6, n, i + 1 + n + n);\n","        # new_data = ndimage.rotate(rec_img.cpu().squeeze().numpy().T, angle, reshape=True)\n","        # plt.imshow(rec_img.cpu().reshape(360, 640, 3))\n","        # print(residual_img.size())\n","        reconstructed_img = image_noisy.cpu() + residual_img.cpu()\n","\n","        # ax.imshow((reconstructed_img.view(3, 360, 640).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8));\n","        ax.imshow(normalize_image(reconstructed_img.view(3, 240, 320)).permute(1, 2, 0).cpu().numpy());\n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False);\n","        ax.get_yaxis().set_visible(False); \n","        if i == n//2:\n","            ax.set_title('Reconstructed image', color=\"red\", fontweight=\"bold\")\n","        \n","        ax = plt.subplot(6, n, i + 1 + n + n + n);\n","\n","        # ax.imshow((residual_img.detach().view(3, 360, 640).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8));\n","        ax.imshow(normalize_image(residual_img.view(3, 240, 320)).permute(1, 2, 0).cpu().numpy());\n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False);\n","        ax.get_yaxis().set_visible(False); \n","        if i == n//2:\n","            ax.set_title('Model Residual image', color=\"red\", fontweight=\"bold\")\n","        \n","        ax = plt.subplot(6, n, i + 1 + n + n + n + n);\n","\n","        # ax.imshow((torch.abs(org_img - image_noisy).detach().view(3, 360, 640).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8));\n","        ax.imshow(normalize_image((org_img - image_noisy).view(3, 240, 320)).permute(1, 2, 0).cpu().numpy());\n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False);\n","        ax.get_yaxis().set_visible(False); \n","        if i == n//2:\n","            ax.set_title('Real Residual image', color=\"red\", fontweight=\"bold\")\n","\n","        ax = plt.subplot(6, n, i + 1 + n + n + n + n + n);\n","\n","        # ax.imshow((torch.abs(org_img - image_noisy).detach().view(3, 360, 640).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8));\n","        ax.imshow(normalize_image(((org_img - image_noisy) - residual_img).view(3, 240, 320)).permute(1, 2, 0).cpu().numpy());\n","        # tr = transforms.Affine2D().rotate_deg(180)\n","        # ax.imshow(img.cpu().squeeze().numpy().T, transform=tr + ax.transData)\n","        ax.get_xaxis().set_visible(False);\n","        ax.get_yaxis().set_visible(False); \n","        if i == n//2:\n","            ax.set_title('Diff Residual image', color=\"red\", fontweight=\"bold\")\n","    \n","    plt.subplots_adjust(left=0.0,\n","                        bottom=0.001, \n","                        right=0.9, \n","                        top=0.999, \n","                        wspace=0., \n","                        hspace=0.1,\n","                        );\n","    plt.show();\n","\n","# plot_ae_outputs_den(my_model=u_net, my_loader=test_loader2, device=device);"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"BlzVOgZPRGEp","executionInfo":{"status":"ok","timestamp":1674755541144,"user_tz":-210,"elapsed":2,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["# import logging\n","# import pytz #import timezone\n","# from datetime import datetime\n","# logging.basicConfig(format='%(asctime)s --- %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.DEBUG)\n","# logger = logging.getLogger('My_Logger')\n","# # logger.setLevel(logging.DEBUG)\n","# tz = pytz.timezone('Asia/Tehran')\n","# def timetz(*args):\n","#     return datetime.now(tz).timetuple()\n","\n","# logging.Formatter.converter = timetz\n","# logger.info('This is an info message')"]},{"cell_type":"markdown","metadata":{"id":"Q3Qo0TJueGfw"},"source":["> # ðŸ”¶ **CheckPointing:**"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"b6J2EBuwNcw-","executionInfo":{"status":"ok","timestamp":1674755543568,"user_tz":-210,"elapsed":3,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["class Checkpoint(object):\n","    def __init__(self, model_name='', checkpoint_path_model=''):\n","        # self.best_acc = 0.0\n","        self.folder = 'checkpoint'\n","        self.model_name = model_name\n","        self.checkpoint_path = str(checkpoint_path_model)\n","        os.makedirs(self.folder, exist_ok=True)\n","\n","    def save(self, model='', epoch=-1, res_train_loss=0, res_val_loss=0, train_loss=0, val_loss=0):\n","        # if acc > self.best_acc:\n","        print('Saving checkpoint...')\n","        state = {\n","            'net': model.state_dict(),\n","            'res_train_loss': res_train_loss,\n","            'res_val_loss': res_val_loss,\n","            'train_loss': train_loss,\n","            'val_loss': val_loss,\n","            'epoch': epoch,\n","            'learning_rate': lr,\n","        }\n","        path = os.path.join(os.path.abspath(self.folder), self.model_name + '_' + str(epoch) +'.pth')\n","        torch.save(state, path)\n","        \n","        str_path = str(path)\n","        # print(str_path)\n","        !cp -av \"$str_path\" \"$self.checkpoint_path\"\n","        # self.best_acc = acc\n","        !rm -rf \"$str_path\"\n","\n","\n","    def load(self, model='', epoch=-1):\n","        drive_PATH = os.path.join(self.checkpoint_path, self.model_name + '_' + str(epoch) +'.pth')\n","        str_drive_PATH = str(drive_PATH)\n","        str_abs_PATH = str(os.path.abspath(self.folder))\n","        print(str_drive_PATH)\n","        print(str_abs_PATH)\n","        !cp -av \"$str_drive_PATH\" '/content/checkpoint'\n","\n","        PATH = os.path.join(os.path.abspath(self.folder), self.model_name + '_' + str(epoch) +'.pth')\n","        str_path = str(PATH)\n","        \n","        checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n","        model.load_state_dict(checkpoint['net'])\n","        # model.eval()\n","        pass\n","my_unet_checkpoint = Checkpoint(model_name='checkpoint_unet', checkpoint_path_model=checkpoint_path_model)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"8SwiTDWuZA8V","executionInfo":{"status":"ok","timestamp":1674755545510,"user_tz":-210,"elapsed":2,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}}},"outputs":[],"source":["class LossHistory(object):\n","    def __init__(self, model_name='U_Net', checkpoint_path_loss=''):\n","        self.folder = 'Loss_History'\n","        self.model_name = model_name\n","        os.makedirs(self.folder, exist_ok=True)\n","        self.checkpoint_path = str(checkpoint_path_loss)\n","        self.file_name = None\n","        self.file_path = None\n","\n","    def save(self, loss_history={}, start_epoch=0, num_epochs=100, epoch=-1):\n","        history_track = {}  # loss history\n","        history_track['train_loss'] = loss_history['train_loss']\n","        history_track['valid_loss'] = loss_history['valid_loss']\n","        history_track['res_train_loss'] = loss_history['res_train_loss']\n","        history_track['res_valid_loss'] = loss_history['res_valid_loss']\n","        history_track['start_epoch'] = start_epoch\n","        history_track['num_epochs'] = num_epochs\n","        history_track['epoch'] = epoch\n","        self.file_name = self.model_name + '_' + 'Loss' + '_' + str(epoch) + '.json'\n","        jason_file = open(os.path.join(os.path.abspath(self.folder), self.file_name), \"w\")\n","        json.dump(history_track, jason_file)\n","        jason_file.close()\n","\n","        self.file_path = os.path.join(os.path.abspath(self.folder), self.file_name)\n","        str_path = str(self.file_path)\n","        # print(str_path)\n","        !cp -av \"$str_path\" \"$self.checkpoint_path\"\n","        # self.best_acc = acc\n","        # !rm -rf \"$str_path\"\n","    \n","    def load(self, epoch=-1):\n","        self.file_name = self.model_name + '_' + 'Loss' + '_' + str(epoch) + '.json'\n","        drive_PATH = os.path.join(self.checkpoint_path, self.file_name)\n","        str_drive_PATH = str(drive_PATH)\n","        str_abs_PATH = str(os.path.abspath(self.folder))\n","\n","        # print(str_drive_PATH)\n","        # print(str_abs_PATH)\n","        !cp -av \"$str_drive_PATH\" \"$str_abs_PATH\"\n","\n","        with open(os.path.join(str_abs_PATH, str(self.file_name)), 'r') as json_file:\n","            json_data = json.load(json_file)\n","            print(\"\\nTrain Loos:=> \", json_data['res_train_loss'])\n","            print(\"\\nTest Loss:=> \", json_data['res_valid_loss'])\n","            print(\"\\nStart Epoch:=> \", json_data['start_epoch'])\n","            print(\"\\nEpoch:=> \", json_data['epoch'])\n","            loss_history = {'train_loss':json_data['train_loss'],\n","                            'valid_loss':json_data['valid_loss'],\n","                            'res_train_loss':json_data['res_train_loss'],\n","                            'res_valid_loss':json_data['res_valid_loss'],\n","                            }\n","            start_epoch = json_data['start_epoch']\n","            num_epochs = json_data['num_epochs']\n","            epoch = json_data['epoch']\n","        return loss_history, start_epoch, num_epochs, epoch    \n","\n","\n","unet_loss_hsitory = LossHistory(model_name='U_Net', checkpoint_path_loss=checkpoint_path_loss)\n","# unet_loss_hsitory.load(epoch=13)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1674755547014,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"},"user_tz":-210},"id":"5OrkytsYXDWB","outputId":"c7c8f0dd-44b0-4f12-cb4a-8ee5ef8d6133"},"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/Models/Checkpoints/MyVideoModel_Final_Residual_V3_BN_Seq/U_Net_Loss_32.json' -> '/content/Loss_History/U_Net_Loss_32.json'\n","\n","Train Loos:=>  [0.028390472754836082, 0.02772160992026329, 0.027273312211036682, 0.02702011913061142, 0.026728685945272446, 0.026557408273220062, 0.026445534080266953, 0.026297053322196007, 0.026169613003730774, 0.026123108342289925, 0.026085108518600464, 0.02608693391084671, 0.02610871195793152, 0.026082297787070274, 0.026006970554590225, 0.025884222239255905, 0.02584070712327957, 0.0257866308093071, 0.02574230171740055, 0.02574824169278145, 0.025628508999943733, 0.02559078484773636, 0.025581305846571922, 0.02555142529308796, 0.025535956025123596, 0.025461411103606224, 0.02548116445541382, 0.02537066675722599, 0.02525990456342697, 0.025181468576192856, 0.025167787447571754, 0.025259699672460556, 0.02511240541934967]\n","\n","Test Loss:=>  [0.7507505416870117, 0.6307492256164551, 0.6038804650306702, 0.6227439045906067, 0.6241623163223267, 0.6377383470535278, 0.6453899145126343, 0.6618392467498779, 0.02939445711672306, 0.02949419803917408, 0.029451582580804825, 0.030084000900387764, 0.029987717047333717, 0.030448684468865395, 0.030789202079176903, 0.03059403970837593, 0.030024223029613495, 0.030036989599466324, 0.030155636370182037, 0.029834507033228874, 0.030219828709959984, 0.030468735843896866, 0.03009689226746559, 0.03025069087743759, 0.05569038167595863, 0.029895074665546417, 0.03027491457760334, 0.04364496096968651, 0.029377780854701996, 0.03132898360490799, 0.033108633011579514, 0.07135793566703796, 0.06266674399375916]\n","\n","Start Epoch:=>  20\n","\n","Epoch:=>  32\n","{'train_loss': [], 'valid_loss': [], 'res_train_loss': [0.028390472754836082, 0.02772160992026329, 0.027273312211036682, 0.02702011913061142, 0.026728685945272446, 0.026557408273220062, 0.026445534080266953, 0.026297053322196007, 0.026169613003730774, 0.026123108342289925, 0.026085108518600464, 0.02608693391084671, 0.02610871195793152, 0.026082297787070274, 0.026006970554590225, 0.025884222239255905, 0.02584070712327957, 0.0257866308093071, 0.02574230171740055, 0.02574824169278145, 0.025628508999943733, 0.02559078484773636, 0.025581305846571922, 0.02555142529308796, 0.025535956025123596, 0.025461411103606224, 0.02548116445541382, 0.02537066675722599, 0.02525990456342697, 0.025181468576192856, 0.025167787447571754, 0.025259699672460556, 0.02511240541934967], 'res_valid_loss': [0.7507505416870117, 0.6307492256164551, 0.6038804650306702, 0.6227439045906067, 0.6241623163223267, 0.6377383470535278, 0.6453899145126343, 0.6618392467498779, 0.02939445711672306, 0.02949419803917408, 0.029451582580804825, 0.030084000900387764, 0.029987717047333717, 0.030448684468865395, 0.030789202079176903, 0.03059403970837593, 0.030024223029613495, 0.030036989599466324, 0.030155636370182037, 0.029834507033228874, 0.030219828709959984, 0.030468735843896866, 0.03009689226746559, 0.03025069087743759, 0.05569038167595863, 0.029895074665546417, 0.03027491457760334, 0.04364496096968651, 0.029377780854701996, 0.03132898360490799, 0.033108633011579514, 0.07135793566703796, 0.06266674399375916]}\n","Start Epoch:===>  33\n","Num epochs:===>  101\n","Epoch:===>  32\n","{'train_loss': [], 'valid_loss': [], 'res_train_loss': [0.028390472754836082, 0.02772160992026329, 0.027273312211036682, 0.02702011913061142, 0.026728685945272446, 0.026557408273220062, 0.026445534080266953, 0.026297053322196007, 0.026169613003730774, 0.026123108342289925, 0.026085108518600464, 0.02608693391084671, 0.02610871195793152, 0.026082297787070274, 0.026006970554590225, 0.025884222239255905, 0.02584070712327957, 0.0257866308093071, 0.02574230171740055, 0.02574824169278145, 0.025628508999943733, 0.02559078484773636, 0.025581305846571922, 0.02555142529308796, 0.025535956025123596, 0.025461411103606224, 0.02548116445541382, 0.02537066675722599, 0.02525990456342697, 0.025181468576192856, 0.025167787447571754, 0.025259699672460556, 0.02511240541934967], 'res_valid_loss': [0.7507505416870117, 0.6307492256164551, 0.6038804650306702, 0.6227439045906067, 0.6241623163223267, 0.6377383470535278, 0.6453899145126343, 0.6618392467498779, 0.02939445711672306, 0.02949419803917408, 0.029451582580804825, 0.030084000900387764, 0.029987717047333717, 0.030448684468865395, 0.030789202079176903, 0.03059403970837593, 0.030024223029613495, 0.030036989599466324, 0.030155636370182037, 0.029834507033228874, 0.030219828709959984, 0.030468735843896866, 0.03009689226746559, 0.03025069087743759, 0.05569038167595863, 0.029895074665546417, 0.03027491457760334, 0.04364496096968651, 0.029377780854701996, 0.03132898360490799, 0.033108633011579514, 0.07135793566703796, 0.06266674399375916]}\n"]}],"source":["start_epoch = 0\n","num_epochs = 101\n","epoch_ = 0\n","loss_history = {}\n","history_da={'train_loss':[],\n","            'valid_loss':[],\n","            'res_train_loss':[],\n","            'res_valid_loss':[],\n","            }\n","\n","if load_losses is True:\n","    loss_history, start_epoch, num_epochs, epoch_ = unet_loss_hsitory.load(epoch=load_epoch)\n","    print(loss_history)\n","    start_epoch = epoch_ + 1\n","\n","    print(\"Start Epoch:===> \", start_epoch)\n","    print(\"Num epochs:===> \", num_epochs)\n","    print(\"Epoch:===> \", epoch_)\n","    history_da['train_loss'] = loss_history['train_loss']\n","    history_da['valid_loss'] = loss_history['valid_loss']\n","    history_da['res_train_loss'] = loss_history['res_train_loss']\n","    history_da['res_valid_loss'] = loss_history['res_valid_loss']\n","    print(history_da)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXkCh5tmvQBK"},"outputs":[],"source":["if load_model is True:\n","    my_unet_checkpoint.load(model=u_net, epoch=load_epoch)\n","\n","plot_ae_outputs_den(my_model=u_net, my_loader=test_loader, device=device);"]},{"cell_type":"markdown","metadata":{"id":"vYXRxk1W9Jrt"},"source":["> # ðŸ”¶ **Training:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JCvzGL91Imj"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","try:\n","  from google import colab\n","  COLAB_ENV = True\n","except (ImportError, ModuleNotFoundError):\n","  COLAB_ENV = False\n","\n","if COLAB_ENV:\n","  %load_ext tensorboard\n","  %tensorboard --logdir runs --host localhost --port 8088\n","else:\n","  print(\"To use tensorboard, please use this notebook in a Google Colab environment!\")\n","\n","writer = SummaryWriter('runs/nature_video')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4Z4McaIvOq4v"},"outputs":[],"source":["### Training cycle\n","\n","for epoch in range(start_epoch, start_epoch + num_epochs):\n","    print('EPOCH %d/%d' % (epoch, start_epoch + num_epochs))\n","    ### Training (use the training function)\n","    res_train_loss = train_epoch_den(\n","        epoch=epoch\n","    )\n","    ### Validation  (use the testing function)\n","    res_val_loss = test_epoch_den(\n","        epoch=epoch\n","    )\n","    # Print Validationloss\n","    # history_da['train_loss'].append(float(train_loss))\n","    # history_da['valid_loss'].append(float(val_loss))\n","    history_da['res_train_loss'].append(float(res_train_loss))\n","    history_da['res_valid_loss'].append(float(res_val_loss))\n","    print('\\n EPOCH {}/{} \\t Residual train loss {:.8f} \\t Residual val loss {:.12f}'.format(epoch, num_epochs, res_train_loss, res_val_loss))\n","\n","    if (epoch % 2) == 0:\n","        plot_ae_outputs_den(my_model=u_net, my_loader=test_loader, device=device)\n","    my_unet_checkpoint.save(model=u_net, epoch=epoch, res_train_loss=res_train_loss, res_val_loss=res_val_loss, train_loss=0, val_loss=0)\n","    unet_loss_hsitory.save(loss_history=history_da, start_epoch=start_epoch, num_epochs=num_epochs, epoch=epoch)"]},{"cell_type":"code","source":["### Training cycle\n","\n","for epoch in range(start_epoch, start_epoch + num_epochs):\n","    print('EPOCH %d/%d' % (epoch, start_epoch + num_epochs))\n","    ### Training (use the training function)\n","    # res_train_loss = train_epoch_den(\n","    #     epoch=epoch\n","    # )\n","    ### Validation  (use the testing function)\n","    res_val_loss = test_epoch_den_time(\n","        epoch=epoch\n","    )\n","    # Print Validationloss\n","    # history_da['train_loss'].append(float(train_loss))\n","    # history_da['valid_loss'].append(float(val_loss))\n","    # history_da['res_train_loss'].append(float(res_train_loss))\n","    # history_da['res_valid_loss'].append(float(res_val_loss))\n","    print('\\n EPOCH {}/{} \\t Residual val loss {:.12f}'.format(epoch, num_epochs, res_val_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fc3912bdcbb945e4ac3e7847e0286a98","c335031a42b7473eb5eed26f9cd6cb67","004e0c6924a342a5a6eb1b3d65d83e03","db585f478b5a47908e89f373a0c6c9e4","947a877d329343ea94447ddfb1441e13","94d6a807b0fd40adb7f1a927c3be6aeb","f37595b11e81424a9676a74d2a573ca1","ada1f9e87d1440c5b9d9321746eeda52","9bb95995d71d4d9dbd050cd46ab0b260","6d44ab11b511438282227bd5b745b53f","964536fb27734e36bed70d1c58705708"]},"id":"876bnQ_mNfDU","executionInfo":{"status":"error","timestamp":1674755639239,"user_tz":-210,"elapsed":12283,"user":{"displayName":"Parham ZM","userId":"12809985044157223796"}},"outputId":"6d272bdd-c1c6-457d-b84b-72b9d3d733ae"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH 33/134\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/6294 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc3912bdcbb945e4ac3e7847e0286a98"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Elapsed time using process_time() 57.58928300000221 ms.\n","Elapsed time using process_time() 50.69194900000085 ms.\n","Elapsed time using process_time() 47.34563099999889 ms.\n","Elapsed time using process_time() 20.332232999997757 ms.\n","Elapsed time using process_time() 24.6354249999996 ms.\n","Elapsed time using process_time() 46.28652699999947 ms.\n","Elapsed time using process_time() 47.309240000000585 ms.\n","Elapsed time using process_time() 46.511857000002266 ms.\n","Elapsed time using process_time() 44.75803799999767 ms.\n","Elapsed time using process_time() 43.710631000003275 ms.\n","Elapsed time using process_time() 48.64800199999664 ms.\n","Elapsed time using process_time() 48.794509000000374 ms.\n","Elapsed time using process_time() 50.35759600000134 ms.\n","Elapsed time using process_time() 48.238791999999364 ms.\n","Elapsed time using process_time() 43.51992700000196 ms.\n","Elapsed time using process_time() 45.41661000000019 ms.\n","Elapsed time using process_time() 44.47845099999981 ms.\n","Elapsed time using process_time() 44.80114999999785 ms.\n","Elapsed time using process_time() 47.23347299999858 ms.\n","Elapsed time using process_time() 46.74410499999837 ms.\n","Elapsed time using process_time() 47.47182300000219 ms.\n","Elapsed time using process_time() 48.01276199999904 ms.\n","Elapsed time using process_time() 47.41506999999956 ms.\n","Elapsed time using process_time() 46.646324000001016 ms.\n","Elapsed time using process_time() 42.89237900000131 ms.\n","Elapsed time using process_time() 45.55452299999985 ms.\n","Elapsed time using process_time() 45.977449999998754 ms.\n","Elapsed time using process_time() 46.60471400000077 ms.\n","Elapsed time using process_time() 43.818828999999226 ms.\n","Elapsed time using process_time() 44.18390900000091 ms.\n","Elapsed time using process_time() 46.767122999998634 ms.\n","Elapsed time using process_time() 44.19007699999966 ms.\n","Elapsed time using process_time() 47.388334000000754 ms.\n","Elapsed time using process_time() 45.04788400000237 ms.\n","Elapsed time using process_time() 46.57006900000127 ms.\n","Elapsed time using process_time() 48.05786799999723 ms.\n","Elapsed time using process_time() 49.083545999998535 ms.\n","Elapsed time using process_time() 46.71853199999987 ms.\n","Elapsed time using process_time() 44.33242100000001 ms.\n","Elapsed time using process_time() 45.37619499999934 ms.\n","Elapsed time using process_time() 46.393209000001434 ms.\n","Elapsed time using process_time() 44.25521299999957 ms.\n","Elapsed time using process_time() 47.07350300000002 ms.\n","Elapsed time using process_time() 44.54332599999944 ms.\n","Elapsed time using process_time() 46.31613300000126 ms.\n","Elapsed time using process_time() 46.27250999999788 ms.\n","Elapsed time using process_time() 48.149055999999746 ms.\n","Elapsed time using process_time() 43.86602199999956 ms.\n","Elapsed time using process_time() 47.11666699999739 ms.\n","Elapsed time using process_time() 45.59623600000151 ms.\n","Elapsed time using process_time() 42.32665900000043 ms.\n","Elapsed time using process_time() 45.86455300000125 ms.\n","Elapsed time using process_time() 45.45742499999861 ms.\n","Elapsed time using process_time() 44.30097200000205 ms.\n","Elapsed time using process_time() 43.55294800000209 ms.\n","Elapsed time using process_time() 46.40462099999709 ms.\n","Elapsed time using process_time() 46.41341700000012 ms.\n","Elapsed time using process_time() 45.28343200000151 ms.\n","Elapsed time using process_time() 44.16572199999891 ms.\n","Elapsed time using process_time() 45.595584999997385 ms.\n","Elapsed time using process_time() 44.22183099999799 ms.\n","Elapsed time using process_time() 42.824461000002145 ms.\n","Elapsed time using process_time() 43.11997200000306 ms.\n","Elapsed time using process_time() 47.22001400000053 ms.\n","Elapsed time using process_time() 45.61946800000172 ms.\n","Elapsed time using process_time() 48.80156600000163 ms.\n","Elapsed time using process_time() 46.914535000002644 ms.\n","Elapsed time using process_time() 45.72471799999889 ms.\n","Elapsed time using process_time() 43.97974700000162 ms.\n","Elapsed time using process_time() 44.87931199999906 ms.\n","Elapsed time using process_time() 42.969742000000366 ms.\n","Elapsed time using process_time() 43.161974000000214 ms.\n","Elapsed time using process_time() 42.72706100000079 ms.\n","Elapsed time using process_time() 46.153418999999474 ms.\n","Elapsed time using process_time() 45.51441700000325 ms.\n","Elapsed time using process_time() 46.10046999999895 ms.\n","Elapsed time using process_time() 44.39081700000003 ms.\n","Elapsed time using process_time() 44.53191599999684 ms.\n","Elapsed time using process_time() 42.91654900000097 ms.\n","Elapsed time using process_time() 43.73661400000017 ms.\n","Elapsed time using process_time() 43.25898800000161 ms.\n","Elapsed time using process_time() 46.047665999999765 ms.\n","Elapsed time using process_time() 46.87593799999945 ms.\n","Elapsed time using process_time() 45.57293499999915 ms.\n","Elapsed time using process_time() 46.236581999998805 ms.\n","Elapsed time using process_time() 47.16583600000135 ms.\n","Elapsed time using process_time() 45.9013370000001 ms.\n","Elapsed time using process_time() 47.89683299999936 ms.\n","Elapsed time using process_time() 48.097986000001924 ms.\n","Elapsed time using process_time() 45.52910800000021 ms.\n","Elapsed time using process_time() 45.447767999998945 ms.\n","Elapsed time using process_time() 47.03124099999911 ms.\n","Elapsed time using process_time() 44.15817599999983 ms.\n","Elapsed time using process_time() 45.77483599999965 ms.\n","Elapsed time using process_time() 43.88988699999885 ms.\n","Elapsed time using process_time() 49.733623000001614 ms.\n","Elapsed time using process_time() 47.64001699999909 ms.\n","Elapsed time using process_time() 48.81994700000192 ms.\n","Elapsed time using process_time() 46.586850000000624 ms.\n","Elapsed time using process_time() 44.10743299999709 ms.\n","Elapsed time using process_time() 49.958121999999605 ms.\n","Elapsed time using process_time() 47.4114280000002 ms.\n","Elapsed time using process_time() 44.42354300000062 ms.\n","Elapsed time using process_time() 43.66129400000318 ms.\n","Elapsed time using process_time() 44.681107999998915 ms.\n","Elapsed time using process_time() 43.482824999998115 ms.\n","Elapsed time using process_time() 45.879791999997366 ms.\n","Elapsed time using process_time() 44.76117600000151 ms.\n","Elapsed time using process_time() 45.292417999998946 ms.\n","Elapsed time using process_time() 44.95282899999964 ms.\n","Elapsed time using process_time() 45.1017130000011 ms.\n","Elapsed time using process_time() 43.414957999999615 ms.\n","Elapsed time using process_time() 47.72287600000169 ms.\n","Elapsed time using process_time() 44.52058900000111 ms.\n","Elapsed time using process_time() 46.549056000003475 ms.\n","Elapsed time using process_time() 43.38920400000035 ms.\n","Elapsed time using process_time() 47.934717999996934 ms.\n","Elapsed time using process_time() 44.478976000000614 ms.\n","Elapsed time using process_time() 46.3585170000016 ms.\n","Elapsed time using process_time() 43.874708000000595 ms.\n","Elapsed time using process_time() 46.071385999997716 ms.\n","Elapsed time using process_time() 45.69967899999838 ms.\n","Elapsed time using process_time() 42.60790700000072 ms.\n","Elapsed time using process_time() 47.3274289999992 ms.\n","Elapsed time using process_time() 47.10765399999772 ms.\n","Elapsed time using process_time() 45.65872499999912 ms.\n","Elapsed time using process_time() 49.13401199999967 ms.\n","Elapsed time using process_time() 47.16582900000077 ms.\n","Elapsed time using process_time() 47.045420999999976 ms.\n","Elapsed time using process_time() 47.53512699999973 ms.\n","Elapsed time using process_time() 49.75827999999893 ms.\n","Elapsed time using process_time() 45.99075799999852 ms.\n","Elapsed time using process_time() 42.979598999998814 ms.\n","Elapsed time using process_time() 44.564226999998624 ms.\n","Elapsed time using process_time() 45.908712000002794 ms.\n","Elapsed time using process_time() 45.56598699999981 ms.\n","Elapsed time using process_time() 45.18469700000338 ms.\n","Elapsed time using process_time() 46.43233499999866 ms.\n","Elapsed time using process_time() 45.92616300000074 ms.\n","Elapsed time using process_time() 46.326232000001966 ms.\n","Elapsed time using process_time() 42.486365000002024 ms.\n","Elapsed time using process_time() 46.72153899999998 ms.\n","Elapsed time using process_time() 47.3458500000028 ms.\n","Elapsed time using process_time() 47.21276000000074 ms.\n","Elapsed time using process_time() 47.52163699999912 ms.\n","Elapsed time using process_time() 45.78418699999531 ms.\n","Elapsed time using process_time() 47.41248999999925 ms.\n","Elapsed time using process_time() 43.41490600000242 ms.\n","Elapsed time using process_time() 45.19525999999985 ms.\n","Elapsed time using process_time() 47.042869000001986 ms.\n","Elapsed time using process_time() 43.863715000000525 ms.\n","Elapsed time using process_time() 46.69228099999856 ms.\n","Elapsed time using process_time() 46.028978000002496 ms.\n","Elapsed time using process_time() 47.21762799999851 ms.\n","Elapsed time using process_time() 46.06559300000157 ms.\n","Elapsed time using process_time() 47.11349400000131 ms.\n","Elapsed time using process_time() 45.79858299999984 ms.\n","Elapsed time using process_time() 47.12398200000223 ms.\n","Elapsed time using process_time() 44.44168900000278 ms.\n","Elapsed time using process_time() 44.15461300000345 ms.\n","Elapsed time using process_time() 43.030966999999976 ms.\n","Elapsed time using process_time() 48.78204199999914 ms.\n","Elapsed time using process_time() 43.66528999999986 ms.\n","Elapsed time using process_time() 47.30619000000047 ms.\n","Elapsed time using process_time() 43.039124999999956 ms.\n","Elapsed time using process_time() 47.72991399999427 ms.\n","Elapsed time using process_time() 47.39231799999999 ms.\n","Elapsed time using process_time() 44.53993300000292 ms.\n","Elapsed time using process_time() 44.096467999999334 ms.\n","Elapsed time using process_time() 48.549204000003954 ms.\n","Elapsed time using process_time() 46.502119999999536 ms.\n","Elapsed time using process_time() 43.95433699999529 ms.\n","Elapsed time using process_time() 46.567962999993995 ms.\n","Elapsed time using process_time() 46.84611199999722 ms.\n","Elapsed time using process_time() 48.576465000003566 ms.\n","Elapsed time using process_time() 47.117403000001445 ms.\n","Elapsed time using process_time() 45.20943299999658 ms.\n","Elapsed time using process_time() 45.63512800000069 ms.\n","Elapsed time using process_time() 45.544425000002775 ms.\n","Elapsed time using process_time() 48.513743000000886 ms.\n","Elapsed time using process_time() 46.5227540000015 ms.\n","Elapsed time using process_time() 47.290850999999634 ms.\n","Elapsed time using process_time() 46.96769400000278 ms.\n","Elapsed time using process_time() 43.67305299999913 ms.\n","Elapsed time using process_time() 43.60306899999955 ms.\n","Elapsed time using process_time() 45.45827900000177 ms.\n","Elapsed time using process_time() 45.48607999999632 ms.\n","Elapsed time using process_time() 45.542871999998624 ms.\n","Elapsed time using process_time() 43.56945100000331 ms.\n","Elapsed time using process_time() 48.157965999998 ms.\n","Elapsed time using process_time() 45.651538000001324 ms.\n","Elapsed time using process_time() 47.250050000002375 ms.\n","Elapsed time using process_time() 43.88683799999882 ms.\n","Elapsed time using process_time() 43.87732899999719 ms.\n","Elapsed time using process_time() 46.61485400000487 ms.\n","Elapsed time using process_time() 43.619270000000654 ms.\n","Elapsed time using process_time() 47.1647130000008 ms.\n","Elapsed time using process_time() 43.095669999999586 ms.\n","Elapsed time using process_time() 43.21040000000664 ms.\n","Elapsed time using process_time() 44.62060200000195 ms.\n","Elapsed time using process_time() 47.57992600000449 ms.\n","Elapsed time using process_time() 44.72366199999556 ms.\n","Elapsed time using process_time() 46.44550000000436 ms.\n","Elapsed time using process_time() 46.7741559999979 ms.\n","Elapsed time using process_time() 44.00796699999887 ms.\n","Elapsed time using process_time() 43.27279900000036 ms.\n","Elapsed time using process_time() 46.464430999996864 ms.\n","Elapsed time using process_time() 46.26530999999545 ms.\n","Elapsed time using process_time() 47.96360600000327 ms.\n","Elapsed time using process_time() 44.58118399999478 ms.\n","Elapsed time using process_time() 45.53761100000031 ms.\n","Elapsed time using process_time() 46.36331900000101 ms.\n","Elapsed time using process_time() 43.836312999999905 ms.\n","Elapsed time using process_time() 47.97048599999698 ms.\n","Elapsed time using process_time() 45.032728999998994 ms.\n","Elapsed time using process_time() 44.2015770000026 ms.\n","Elapsed time using process_time() 42.51269699999938 ms.\n","Elapsed time using process_time() 43.55160800000135 ms.\n","Elapsed time using process_time() 43.78904899999725 ms.\n","Elapsed time using process_time() 42.559793999998874 ms.\n","Elapsed time using process_time() 45.226660000004415 ms.\n","Elapsed time using process_time() 48.44497799999914 ms.\n","Elapsed time using process_time() 47.506709000003866 ms.\n","Elapsed time using process_time() 47.610300999998856 ms.\n","Elapsed time using process_time() 44.48297100000076 ms.\n","Elapsed time using process_time() 46.80810600000029 ms.\n","Elapsed time using process_time() 43.6499289999972 ms.\n","Elapsed time using process_time() 45.501419999993686 ms.\n","Elapsed time using process_time() 45.50938699999563 ms.\n","Elapsed time using process_time() 47.28766000000206 ms.\n","Elapsed time using process_time() 43.66922799999884 ms.\n","Elapsed time using process_time() 45.643728999998245 ms.\n","Elapsed time using process_time() 44.449136999993755 ms.\n","Elapsed time using process_time() 47.38148599999903 ms.\n","Elapsed time using process_time() 44.673099999997135 ms.\n","Elapsed time using process_time() 44.04698999999823 ms.\n","Elapsed time using process_time() 44.25224999999955 ms.\n","Elapsed time using process_time() 47.74345300000249 ms.\n","Elapsed time using process_time() 44.95603999999531 ms.\n","Elapsed time using process_time() 45.0412579999977 ms.\n","Elapsed time using process_time() 47.05223199999864 ms.\n","Elapsed time using process_time() 43.657333999995274 ms.\n","Elapsed time using process_time() 46.02101999999775 ms.\n","Elapsed time using process_time() 46.897424999997384 ms.\n","Elapsed time using process_time() 44.904142000000036 ms.\n","Elapsed time using process_time() 44.25982600000111 ms.\n","Elapsed time using process_time() 43.084978000003105 ms.\n","Elapsed time using process_time() 46.286241000004225 ms.\n","Elapsed time using process_time() 45.772655000000384 ms.\n","Elapsed time using process_time() 46.58658299999985 ms.\n","Elapsed time using process_time() 44.446075999999834 ms.\n","Elapsed time using process_time() 45.891120000000285 ms.\n","Elapsed time using process_time() 42.01696100000163 ms.\n","Elapsed time using process_time() 43.782718999999304 ms.\n","Elapsed time using process_time() 43.19979899999993 ms.\n","Elapsed time using process_time() 45.43969300000583 ms.\n","Elapsed time using process_time() 47.86020600000285 ms.\n","Elapsed time using process_time() 47.956568000003585 ms.\n","Elapsed time using process_time() 45.53745300000145 ms.\n","Elapsed time using process_time() 44.44902099999837 ms.\n","Elapsed time using process_time() 46.92981899999893 ms.\n","Elapsed time using process_time() 45.35040900000098 ms.\n","Elapsed time using process_time() 42.8427109999987 ms.\n","Elapsed time using process_time() 44.09764700000096 ms.\n","Elapsed time using process_time() 44.23076700000195 ms.\n","Elapsed time using process_time() 44.86817100000451 ms.\n","Elapsed time using process_time() 47.56600500000019 ms.\n","Elapsed time using process_time() 48.578142000003766 ms.\n","Elapsed time using process_time() 48.17515900000302 ms.\n","Elapsed time using process_time() 46.71084299999961 ms.\n","Elapsed time using process_time() 46.277074999999 ms.\n","Elapsed time using process_time() 45.97678300000041 ms.\n","Elapsed time using process_time() 47.26299300000392 ms.\n","Elapsed time using process_time() 48.167701000004115 ms.\n","Elapsed time using process_time() 47.001654999995424 ms.\n","Elapsed time using process_time() 48.87802900000082 ms.\n","Elapsed time using process_time() 48.042958000003466 ms.\n","Elapsed time using process_time() 44.7059720000027 ms.\n","Elapsed time using process_time() 43.85796199999703 ms.\n","Elapsed time using process_time() 45.298923000004265 ms.\n","Elapsed time using process_time() 45.33150199999625 ms.\n","Elapsed time using process_time() 46.63287500000024 ms.\n","Elapsed time using process_time() 46.52608600000008 ms.\n","Elapsed time using process_time() 45.61856899999839 ms.\n","Elapsed time using process_time() 45.096669000002976 ms.\n","Elapsed time using process_time() 44.787988000003054 ms.\n","Elapsed time using process_time() 47.91557899999788 ms.\n","Elapsed time using process_time() 44.213233999997215 ms.\n","Elapsed time using process_time() 48.309848999998906 ms.\n","Elapsed time using process_time() 47.68215300000378 ms.\n","Elapsed time using process_time() 47.87516100000033 ms.\n","Elapsed time using process_time() 47.26271600000587 ms.\n","Elapsed time using process_time() 46.47163300000301 ms.\n","Elapsed time using process_time() 47.03100100000057 ms.\n","Elapsed time using process_time() 45.027566999998214 ms.\n","Elapsed time using process_time() 47.16251299999641 ms.\n","Elapsed time using process_time() 43.67615299999983 ms.\n","Elapsed time using process_time() 46.85848199999754 ms.\n","Elapsed time using process_time() 48.85562199999782 ms.\n","Elapsed time using process_time() 46.4021009999982 ms.\n","Elapsed time using process_time() 48.50672599999939 ms.\n","Elapsed time using process_time() 45.61672899999536 ms.\n","Elapsed time using process_time() 47.45412499999446 ms.\n","Elapsed time using process_time() 47.06231099999769 ms.\n","Elapsed time using process_time() 45.15846700000026 ms.\n","Elapsed time using process_time() 45.12053999999921 ms.\n","Elapsed time using process_time() 44.1489500000003 ms.\n","Elapsed time using process_time() 45.503902000000096 ms.\n","Elapsed time using process_time() 47.878672999999594 ms.\n","Elapsed time using process_time() 46.94896200000187 ms.\n","Elapsed time using process_time() 44.98065899999659 ms.\n","Elapsed time using process_time() 46.066165000006265 ms.\n","Elapsed time using process_time() 48.58131200000315 ms.\n","Elapsed time using process_time() 45.10110900000086 ms.\n","Elapsed time using process_time() 44.87713299999996 ms.\n","Elapsed time using process_time() 46.688570999997125 ms.\n","Elapsed time using process_time() 44.16893500000185 ms.\n","Elapsed time using process_time() 45.892650000006086 ms.\n","Elapsed time using process_time() 45.70032099999821 ms.\n","Elapsed time using process_time() 49.26867500000043 ms.\n","Elapsed time using process_time() 45.75707900000481 ms.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-7647797b9ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m### Validation  (use the testing function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     res_val_loss = test_epoch_den_time(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n","\u001b[0;32m<ipython-input-28-ba30a84edb8b>\u001b[0m in \u001b[0;36mtest_epoch_den_time\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Encode data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdecoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmy_pre_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;31m# res_loss = res_loss_fn_mae((decoded_data + noisy_batch + my_pre_x), raw_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmy_pre_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-3bbd841dd4a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0my7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0my8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0my9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0my10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0my11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-2cbc4b40b6a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         '''\n\u001b[1;32m     68\u001b[0m         \u001b[0moriginal_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n\u001b[0m\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rm1agsVwYZf2"},"outputs":[],"source":["fig = plt.figure(figsize=(10, 7));\n","# plt.plot(history_da['res_train_loss'][0:24], label='Res-Train loss')\n","valid_losses = history_da['res_valid_loss'][:24]\n","train_losses = history_da['res_train_loss'][:24]\n","initialize_loss1 = 0.036506280303001404\n","initialize_loss2 = 0.036506280303001404\n","valid_losses.insert(0, initialize_loss1)\n","train_losses.insert(0, initialize_loss2)\n","plt.plot(train_losses, label='Res-Train loss')\n","plt.plot(valid_losses, label='Res-Validation loss')\n","plt.axhline(y=0.0, label='Zero Test Loss', c='red')\n","\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Residual Loss\")\n","plt.title(\"Calculated Res-Losses for ADE\", fontsize=15)\n","plt.legend();\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"VwQXunoQLGWS"},"source":["# **Reconstructing Video:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEAQXrbB0I0R"},"outputs":[],"source":["from torchvision.utils import save_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-yfugay0evh"},"outputs":[],"source":["!mkdir '/content/Frames/ReConstructedFrames/'\n","!mkdir '/content/Frames/ReConstructedFrames/Train/'\n","!mkdir '/content/Frames/ReConstructedFrames/Test/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2gvk5dfnKI4"},"outputs":[],"source":["save_path_test = '/content/Frames/ReConstructedFrames/Test/'\n","save_path_train = '/content/Frames/ReConstructedFrames/Train/'\n","\n","train_loader_1 = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n","test_loader_1 = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtpOB8Q3OjeG"},"outputs":[],"source":["### Training function\n","def make_model_train_frames():\n","    # Set train mode for both the encoder and the decoder\n","    u_net.eval()\n","\n","    with torch.no_grad(): # No need to track the gradients\n","        # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","        for iter, (batch) in enumerate(tqdm(train_loader_1)): # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n","            file_name_number = batch['CompName'][0][9:14]\n","            file_name = \"recframe\" + str(file_name_number) + \".png\"\n","            # Move tensor to the proper device\n","            noisy_batch = batch['CompIMG'].to(device)\n","            # U-Net data\n","            decoded_data = u_net(noisy_batch)\n","            # Evaluate loss\n","            rec_image = decoded_data + noisy_batch\n","            save_image((rec_image.view(3, 240, 320)), str(save_path_train + file_name))\n","\n","\n","### Testing function\n","def make_model_test_frames():\n","    # Set evaluation mode for encoder and decoder\n","    u_net.eval()\n","\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        # with tqdm(test_loader, unit=\"batch\") as tepoch:\n","        for iter, (batch) in enumerate(tqdm(test_loader_1)): #zip(tqdm(test_decoded_loader), test_raw_loader):\n","            file_name_number = batch['CompName'][0][9:14]\n","            file_name = \"recframe\" + str(file_name_number) + \".png\"\n","            # Move tensor to the proper device\n","            noisy_batch = batch['CompIMG'].to(device)\n","            # U-Net data\n","            decoded_data = u_net(noisy_batch)\n","            # Evaluate loss\n","            rec_image = decoded_data + noisy_batch\n","            save_image((rec_image.view(3, 240, 320)), str(save_path_test + file_name))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Dhq_fStltUJ"},"outputs":[],"source":["make_model_train_frames()\n","make_model_test_frames()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQVRHG0V3PI-"},"outputs":[],"source":["!zip -r \"./ReConstructedFrames_240p_6Frames_CRF40_13_PNG_RGB_Slow.zip\" \"./Frames/ReConstructedFrames\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"le2_jUiZ6obF"},"outputs":[],"source":["!cp -av './ReConstructedFrames_240p_6Frames_CRF40_13_PNG_RGB_Slow.zip' '/content/drive/Shareddrives/AI/Datasets/ExtractedFrames/Nature_Frames2/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3jMUKtgOs_Q"},"outputs":[],"source":["# img_noise = iter(noise_dataloader).next()\n","# img = iter(dataloader_train).next()\n","\n","\n","# # import matplotlib.pyplot as plt\n","# # a = np.array([1,2,3,4,5])\n","# # a = np.expand_dims(a, axis=0)  # or axis=1\n","# # plt.imshow(a)\n","# # plt.show()\n","# # with torch.no_grad():\n","# #     rec_img_noise  = decoder(encoder(img_noise[0]))\n","\n","\n","\n","# import numpy as np\n","\n","# def valid_imshow_data(data):\n","#     data = np.asarray(data)\n","#     if data.ndim == 2:\n","#         return True\n","#     elif data.ndim == 3:\n","#         if 3 <= data.shape[2] <= 4:\n","#             return True\n","#         else:\n","#             print('The \"data\" has 3 dimensions but the last dimension '\n","#                   'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n","#                   ''.format(data.shape[2]))\n","#             return False\n","#     else:\n","#         print('To visualize an image the data must be 2 dimensional or '\n","#               '3 dimensional, not \"{}\".'\n","#               ''.format(data.ndim))\n","#         return False\n","\n","# print(img_noise[0][0].shape)\n","# a = valid_imshow_data(img_noise[0][0])\n","\n","\n","# #     rec_img  = img[0]))\n","# plt.imshow(torch.squeeze(img_noise[0][0]).reshape(288, 352, 3))\n","# # rec_img_noise(img[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK_lrrBb9sar"},"outputs":[],"source":["# my_loss = nn.MSELoss()\n","\n","# no_dae_loss = []\n","# for (noise_batch, _), (image_batch, _) in zip(train_decoded_loader, train_raw_loader):\n","#     # Move tensor to the proper device\n","#     # image_noisy = add_noise(image_batch,noise_factor)\n","#     # image_noisy = noise_batch.to(device)\n","#     # Encode data\n","#     # encoded_data = encoder(image_noisy)\n","#     # Decode data\n","#     # decoded_data = decoder(encoded_data)\n","#     # Append the network output and the original image to the lists\n","#     # conc_out.append(decoded_data.cpu())\n","#     # conc_label.append(image_batch.cpu())\n","#     loss = my_loss(noise_batch.cpu(), image_batch.cpu())\n","#     no_dae_loss.append(loss.detach().cpu().numpy())\n","\n","# print(\"Train Loss without DAE:=> \", np.mean(no_dae_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfPswEvyaoWN"},"outputs":[],"source":["# my_loss = nn.MSELoss()\n","\n","# no_dae_loss = []\n","# for (noise_batch, _), (image_batch, _) in zip(test_decoded_loader, test_raw_loader):\n","#     # Move tensor to the proper device\n","#     # image_noisy = add_noise(image_batch,noise_factor)\n","#     # image_noisy = noise_batch.to(device)\n","#     # Encode data\n","#     # encoded_data = encoder(image_noisy)\n","#     # Decode data\n","#     # decoded_data = decoder(encoded_data)\n","#     # Append the network output and the original image to the lists\n","#     # conc_out.append(decoded_data.cpu())\n","#     # conc_label.append(image_batch.cpu())\n","#     loss = my_loss(noise_batch.cpu(), image_batch.cpu())\n","#     no_dae_loss.append(loss.detach().cpu().numpy())\n","\n","# print(\"Test Loss without DAE:=> {:.12f}\".format(np.mean(no_dae_loss)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Sa28KxIG3gO"},"outputs":[],"source":["batch1_decoded, batch_raw = next(iter(train_loader))\n","\n","plt.imshow(batch1_decoded[7].view(3, 256, 512).permute(1, 2, 0).detach().cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6jQw4-FOFTe"},"outputs":[],"source":["\n","plt.imshow(batch1_decoded[7].view(3, 256, 512).permute(1, 2, 0).detach().cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro4h9cXdLIcu"},"outputs":[],"source":["torch.cuda.empty_cache()\n","del encoded_data\n","del decoded_data\n","del test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c563mP3J5t2"},"outputs":[],"source":["encoded_data = u_net(batch1_decoded[7].view(1, 3, 256, 512).to(device))\n","# Decode data\n","# decoded_data = decoder(encoded_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YahoSARE6ke6"},"outputs":[],"source":["print(decoded_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dioqsIKYKdny"},"outputs":[],"source":["reconstructed_img = batch1_decoded[7].detach().cpu() + decoded_data.detach().cpu()\n","plt.imshow(reconstructed_img.view(3, 256, 512).permute(1, 2, 0).numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeK6kxkL8rLR"},"outputs":[],"source":["from torchvision.utils import save_image\n","save_image(batch1_decoded[7], \"noisy.bmp\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0VEs64J9S7R"},"outputs":[],"source":["save_image(reconstructed_img, \"reconstructed_img.bmp\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RicTCCl49ZL6"},"outputs":[],"source":["\n","save_image(batch_raw[7], \"raw_img.bmp\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QpZdJt1Kmfw"},"outputs":[],"source":["import cv2\n","# img       = 255 - cv2.resize(batch1_decoded[7],       (0,0), fx = 1/scale_w, fy = 1/scale_h)\n","# cv2.imwrite(os.path.join('/content', 'img', 'noisy', '{}.jpg'.format(str(1).zfill(6))), img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qU45AGapQvC"},"outputs":[],"source":["class ImageFolderNew(ImageFolder):\n","    def _find_classes(self, dir):\n","        \"\"\"\n","        Finds the class folders in a dataset.\n","\n","        Args:\n","            dir (string): Root directory path.\n","\n","        Returns:\n","            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n","\n","        Ensures:\n","            No class is a subdirectory of another.\n","        \"\"\"\n","        if sys.version_info >= (3, 5):\n","            # Faster and available in Python 3.5 and above\n","            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n","        else:\n","            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n","        classes.sort(key=int)\n","        class_to_idx = {classes[i]: i for i in range(len(classes))}\n","        return classes, class_to_idx\n","\n","\n","class MyDataset(Dataset):\n","    def __init__(self, datasetA_dir, datasetB_dir, transform=None):\n","        self.datasetA_dir = datasetA_dir\n","        self.datasetB_dir = datasetB_dir\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        # print(\"index: \", index)\n","        # print(\"dirA: \", self.datasetA_dir)\n","        # print(\"dirB: \", self.datasetB_dir)\n","        # print(\"LenA: \", len(self.datasetA_dir))\n","        # print(\"LenB: \", len(self.datasetB_dir))\n","        digA = \"compframe00315\"  + \".png\"\n","        dirA = os.path.join(self.datasetA_dir, digA)\n","        digB = \"rawframe00315\" + \".png\"\n","        dirB = os.path.join(self.datasetB_dir, digB)\n","        # xA = self.datasetA_dir[index]\n","        # xB = self.datasetB_dir[index]\n","        imageA = io.imread(dirA)\n","        imageB = io.imread(dirB)\n","        if self.transform:\n","            imageA = self.transform(imageA)\n","            imageB = self.transform(imageB)\n","        return imageA, imageB\n","    \n","    def __len__(self):\n","        path, dirs, filesA = next(os.walk(self.datasetA_dir))\n","        # path, dirs, filesB = next(os.walk(self.datasetB_dir))\n","        file_countA = len(filesA)\n","        # file_countB = len(filesB)\n","        # print(\"File CountA: \", file_countA)\n","        # print(\"File CountB: \", file_countB)\n","        # print(\"Len Decoded:\", len(self.datasetA_dir))\n","        # print(\"Len Raw:\", len(self.datasetB_dir))\n","        return file_countA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLd1l_Tlmrio"},"outputs":[],"source":["test_dataset2 = MyDataset('/content/Frame2/dec/', '/content/Frame2/raw/', transform=transforms.Compose([\n","                                transforms.ToTensor(),\n","                                torchvision.transforms.Normalize(\n","                                  mean=[0.5, 0.5, 0.5],\n","                                  std=[0.5, 0.5, 0.5],\n","                              ),\n","                            ])\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ag-zv-VOOmwP"},"outputs":[],"source":["test_loader2 = torch.utils.data.DataLoader(test_dataset2, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo_fQjSynWfH"},"outputs":[],"source":["image_noisy, org_img = next(iter(test_loader2))\n","\n","image_noisy = image_noisy.to(device)\n","residual_img = u_net(image_noisy.view(1, 3, 360, 640))\n","\n","# res = u_net(test_loader2)\n","\n","reconstructed_img = image_noisy.detach().cpu() + residual_img.detach().cpu()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["7SnT7YhEJFrG","_ebAc19vIMRb","BgJcAODtJva3","kwsG5HBNToph","r3qmP0qBMzjo","wWaVY_lUM6_4","VwQXunoQLGWS"],"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fc3912bdcbb945e4ac3e7847e0286a98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c335031a42b7473eb5eed26f9cd6cb67","IPY_MODEL_004e0c6924a342a5a6eb1b3d65d83e03","IPY_MODEL_db585f478b5a47908e89f373a0c6c9e4"],"layout":"IPY_MODEL_947a877d329343ea94447ddfb1441e13"}},"c335031a42b7473eb5eed26f9cd6cb67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94d6a807b0fd40adb7f1a927c3be6aeb","placeholder":"â€‹","style":"IPY_MODEL_f37595b11e81424a9676a74d2a573ca1","value":"  5%"}},"004e0c6924a342a5a6eb1b3d65d83e03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_ada1f9e87d1440c5b9d9321746eeda52","max":6294,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bb95995d71d4d9dbd050cd46ab0b260","value":321}},"db585f478b5a47908e89f373a0c6c9e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d44ab11b511438282227bd5b745b53f","placeholder":"â€‹","style":"IPY_MODEL_964536fb27734e36bed70d1c58705708","value":" 321/6294 [00:09&lt;02:59, 33.34it/s]"}},"947a877d329343ea94447ddfb1441e13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d6a807b0fd40adb7f1a927c3be6aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f37595b11e81424a9676a74d2a573ca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ada1f9e87d1440c5b9d9321746eeda52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bb95995d71d4d9dbd050cd46ab0b260":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d44ab11b511438282227bd5b745b53f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"964536fb27734e36bed70d1c58705708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}